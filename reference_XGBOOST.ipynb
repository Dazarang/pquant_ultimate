{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Libraries\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import requests_cache\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from plotly.subplots import make_subplots\n",
    "from pybroker import YFinance, StrategyConfig, Strategy\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import indicators  # own .py file\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Fetch data\n",
    "# %%\n",
    "import yfinance as yf\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "TICKERS = [\"AAPL\", \"EVO.ST\", \"HM-B.ST\", \"CRM\", \"IBM\", \"ASSA-B.ST\"]\n",
    "\n",
    "\n",
    "session = requests_cache.CachedSession(\"magic/.pybrokercache/yfinance.cache\")\n",
    "session.headers[\"User-agent\"] = \"my-program/1.0\"\n",
    "\n",
    "\n",
    "# Get today's date\n",
    "date = datetime.today() - timedelta(days=1)\n",
    "\n",
    "# # Format date as D/M/YYYY\n",
    "date_string = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "data = yf.download(\n",
    "    TICKERS, start=\"2018-01-01\", end=date_string, group_by=\"ticker\", session=session\n",
    ")\n",
    "\n",
    "\n",
    "split_data = {}\n",
    "if len(TICKERS) == 1:\n",
    "    split_data[TICKERS[0]] = data\n",
    "    split_data[TICKERS[0]].reset_index(inplace=True)\n",
    "    split_data[TICKERS[0]].columns = [\n",
    "        col.lower() for col in split_data[TICKERS[0]].columns\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    # Split data into individual DataFrames\n",
    "    for ticker in TICKERS:\n",
    "        split_data[ticker] = data[ticker]\n",
    "        # Turn columns to lowercase\n",
    "        split_data[ticker].reset_index(inplace=True)\n",
    "        split_data[ticker].columns = [col.lower() for col in split_data[ticker].columns]\n",
    "\n",
    "\n",
    "# Print number of rows and columns for each DataFrame\n",
    "for ticker in TICKERS:\n",
    "    print(f\"{ticker}: {split_data[ticker].shape}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### TA indicators\n",
    "\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# TICKERS = [\"AAPL\", \"EVO.ST\", \"GOOGL\", \"MSFT\", \"TSLA\"]\n",
    "TICKERS = [\"AAPL\", \"EVO.ST\", \"HM-B.ST\", \"CRM\", \"IBM\", \"ASSA-B.ST\"]\n",
    "# TICKERS = [\"AAPL\", \"EVO.ST\"]\n",
    "# TICKERS = [\"EVO.ST\"]\n",
    "\n",
    "session = requests_cache.CachedSession('magic/.pybrokercache/yfinance.cache')\n",
    "session.headers['User-agent'] = 'my-program/1.0'\n",
    "\n",
    "\n",
    "# Get today's date\n",
    "date = datetime.today() - timedelta(days=1)\n",
    "date\n",
    "# # Format date as D/M/YYYY\n",
    "date_string = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "data = yf.download(TICKERS, start=\"2018-01-01\", end=date_string, group_by=\"ticker\", session=session)\n",
    "\n",
    "\n",
    "split_data = {}\n",
    "if len(TICKERS) == 1:\n",
    "    split_data[TICKERS[0]] = data\n",
    "    split_data[TICKERS[0]].reset_index(inplace=True)\n",
    "    split_data[TICKERS[0]].columns = [col.lower() for col in split_data[TICKERS[0]].columns]\n",
    "\n",
    "else:\n",
    "    # Split data into individual DataFrames\n",
    "    for ticker in TICKERS:\n",
    "        split_data[ticker] = data[ticker]\n",
    "        # Turn columns to lowercase\n",
    "        split_data[ticker].reset_index(inplace=True)\n",
    "        split_data[ticker].columns = [col.lower() for col in split_data[ticker].columns]\n",
    "\n",
    "    # type(split_data[\"AAPL\"])\n",
    "\n",
    "# Print number of rows and columns for each DataFrame\n",
    "for ticker in TICKERS:\n",
    "    print(f\"{ticker}: {split_data[ticker].shape}\")\n",
    "    \n",
    "# split_data[\"EVO.ST\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TA indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate TA indicators\n",
    "def calculate_ta_indicators(data):\n",
    "    data.dropna(inplace=True)\n",
    "    periods = [5, 8, 21, 55, 89, 200]\n",
    "    ema_cols = {}\n",
    "    sma_cols = {}\n",
    "    wma_cols = {}\n",
    "\n",
    "    for period in periods:\n",
    "        ema_cols[f\"EMA_{period}\"] = indicators.calculate_ema(data, period=period)\n",
    "        sma_cols[f\"SMA_{period}\"] = indicators.calculate_sma(data, period=period)\n",
    "        wma_cols[f\"WMA_{period}\"] = indicators.calculate_wma(data, period=period)\n",
    "\n",
    "    # Add moving averages to data\n",
    "    data = pd.concat(\n",
    "        [data, pd.DataFrame(ema_cols), pd.DataFrame(sma_cols), pd.DataFrame(wma_cols)],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Calculate TA's\n",
    "    ta_cols = {\n",
    "        \"BB_upper\": indicators.calculate_bbands(data)[0],\n",
    "        \"BB_middle\": indicators.calculate_bbands(data)[1],\n",
    "        \"BB_lower\": indicators.calculate_bbands(data)[2],\n",
    "        \"ADX\": indicators.calculate_adx(data),\n",
    "        \"MACD\": indicators.calculate_macd(data)[0],\n",
    "        \"MACD_signal\": indicators.calculate_macd(data)[1],\n",
    "        \"MACD_hist\": indicators.calculate_macd(data)[2],\n",
    "        \"RSI\": indicators.calculate_rsi(data),\n",
    "        \"ADR\": indicators.calculate_adr(data),\n",
    "        \"PivotHigh\": indicators.find_pivots(data, return_boolean=True)[0],\n",
    "        \"PivotLow\": indicators.find_pivots(data, return_boolean=True, rb=21, lb=21)[1],\n",
    "        \"RealStrength\": indicators.enhanced_real_time_strength_index(data),\n",
    "        \"SAR\": indicators.calculate_sar(data),\n",
    "        \"OBV\": indicators.calculate_obv(data, ema_period=55)[0],\n",
    "        \"OBV_EMA_55\": indicators.calculate_obv(data, ema_period=55)[1],\n",
    "        \"OBV_EMA_8\": indicators.calculate_obv(data, ema_period=8)[1],\n",
    "        \"OBV_EMA_21\": indicators.calculate_obv(data, ema_period=21)[1],\n",
    "        \"OBV_EMA_144\": indicators.calculate_obv(data, ema_period=144)[1],\n",
    "        \"ATR\": indicators.calculate_atr(data),\n",
    "        \"Hammer\": indicators.calculate_hammer(data),\n",
    "        \"VWAP\": indicators.calculate_vwap(data),\n",
    "        \"ADOSC\": indicators.calculate_adosc(data, fastperiod=3, slowperiod=10),\n",
    "        \"HT_SINE\": indicators.calculate_ht_sine(data)[0],\n",
    "        \"HT_LEADSINE\": indicators.calculate_ht_sine(data)[1],\n",
    "        \"HT_TRENDMODE\": indicators.calculate_ht_trendmode(data),\n",
    "        \"ROC\": indicators.calculate_roc(data, period=10),\n",
    "        \"MOM\": indicators.calculate_mom(data, period=10),\n",
    "        \"APZ_upper\": indicators.calculate_apz(data)[0],\n",
    "        \"APZ_lower\": indicators.calculate_apz(data)[1],\n",
    "    }\n",
    "\n",
    "    # Add TA's to data\n",
    "    data = pd.concat([data, pd.DataFrame(ta_cols)], axis=1)\n",
    "\n",
    "    # Calculate short-term and long-term differences for EMA, SMA and WMA\n",
    "    diff_cols = {\n",
    "        \"SMA_diff_short_5_8\": data[\"SMA_5\"] - data[\"SMA_8\"],\n",
    "        \"SMA_diff_short_8_21\": data[\"SMA_8\"] - data[\"SMA_21\"],\n",
    "        \"SMA_diff_short_21_55\": data[\"SMA_21\"] - data[\"SMA_55\"],\n",
    "        \"EMA_diff_short_5_8\": data[\"EMA_5\"] - data[\"EMA_8\"],\n",
    "        \"EMA_diff_short_8_21\": data[\"EMA_8\"] - data[\"EMA_21\"],\n",
    "        \"EMA_diff_short_21_55\": data[\"EMA_21\"] - data[\"EMA_55\"],\n",
    "        \"WMA_diff_short_5_8\": data[\"WMA_5\"] - data[\"WMA_8\"],\n",
    "        \"WMA_diff_short_8_21\": data[\"WMA_8\"] - data[\"WMA_21\"],\n",
    "        \"WMA_diff_short_21_55\": data[\"WMA_21\"] - data[\"WMA_55\"],\n",
    "        \"SMA_diff_long_55_89\": data[\"SMA_55\"] - data[\"SMA_89\"],\n",
    "        \"SMA_diff_long_55_200\": data[\"SMA_55\"] - data[\"SMA_200\"],\n",
    "        \"EMA_diff_long_55_89\": data[\"EMA_55\"] - data[\"EMA_89\"],\n",
    "        \"EMA_diff_long_55_200\": data[\"EMA_55\"] - data[\"EMA_200\"],\n",
    "        \"WMA_diff_long_55_89\": data[\"WMA_55\"] - data[\"WMA_89\"],\n",
    "        \"WMA_diff_long_55_200\": data[\"WMA_55\"] - data[\"WMA_200\"],\n",
    "        \"OBV_diff_short_8_21\": data[\"OBV\"] - data[\"OBV_EMA_8\"],\n",
    "        \"OBV_diff_short_21_55\": data[\"OBV\"] - data[\"OBV_EMA_21\"],\n",
    "        \"OBV_diff_long_55_144\": data[\"OBV\"] - data[\"OBV_EMA_144\"],\n",
    "    }\n",
    "\n",
    "    # Add differences to data\n",
    "    data = pd.concat([data, pd.DataFrame(diff_cols)], axis=1)\n",
    "\n",
    "    # Additional features\n",
    "    data[\"RSI_divergence\"] = data[\"RSI\"] - data[\"RSI\"].rolling(window=5).mean()\n",
    "    data[\"MACD_acceleration\"] = data[\"MACD\"] - data[\"MACD\"].shift(1)\n",
    "    data[\"BB_percentage\"] = (data[\"close\"] - data[\"BB_lower\"]) / (\n",
    "        data[\"BB_upper\"] - data[\"BB_lower\"]\n",
    "    )\n",
    "    data[\"volume_price_trend\"] = data[\"volume\"] * (\n",
    "        data[\"close\"] - data[\"close\"].shift(1)\n",
    "    )\n",
    "    data[\"trend_strength\"] = abs(data[\"EMA_21\"] - data[\"SMA_21\"]) / data[\"SMA_21\"]\n",
    "\n",
    "    return data.copy()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Feauture engineering\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feauture engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform feature engineering\n",
    "def perform_feature_engineering(data):\n",
    "    data = data.copy()\n",
    "    data.set_index(\"date\", inplace=True)\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    # Create Lag Features\n",
    "    lagged_features = []\n",
    "    for col in [\n",
    "        \"open\",\n",
    "        \"high\",\n",
    "        \"low\",\n",
    "        \"close\",\n",
    "        \"volume\",\n",
    "        \"RealStrength\",\n",
    "        \"ADX\",\n",
    "        \"MACD\",\n",
    "        \"MACD_signal\",\n",
    "        \"MACD_hist\",\n",
    "        \"RSI\",\n",
    "        \"ADR\",\n",
    "        \"BB_upper\",\n",
    "        \"BB_middle\",\n",
    "        \"BB_lower\",\n",
    "        \"APZ_upper\",\n",
    "        \"APZ_lower\",\n",
    "        \"SAR\",\n",
    "        \"OBV\",\n",
    "        \"ATR\",\n",
    "        \"Hammer\",\n",
    "        \"VWAP\",\n",
    "        \"OBV_EMA_21\",\n",
    "        \"OBV_EMA_55\",\n",
    "        \"OBV_diff_short_21_55\",\n",
    "        \"OBV_diff_short_8_21\",\n",
    "        \"OBV_diff_long_55_144\",\n",
    "        \"ADOSC\",\n",
    "        \"HT_SINE\",\n",
    "        \"HT_LEADSINE\",\n",
    "        \"HT_TRENDMODE\",\n",
    "        \"ROC\",\n",
    "        \"MOM\",\n",
    "        \"SMA_5\",\n",
    "        \"SMA_8\",\n",
    "        \"SMA_21\",\n",
    "        \"SMA_55\",\n",
    "        \"SMA_89\",\n",
    "        \"SMA_200\",\n",
    "        \"EMA_5\",\n",
    "        \"EMA_8\",\n",
    "        \"EMA_21\",\n",
    "        \"EMA_55\",\n",
    "        \"EMA_89\",\n",
    "        \"EMA_200\",\n",
    "        \"WMA_5\",\n",
    "        \"WMA_8\",\n",
    "        \"WMA_21\",\n",
    "        \"WMA_55\",\n",
    "        \"WMA_89\",\n",
    "        \"WMA_200\",\n",
    "        \"SMA_diff_short_5_8\",\n",
    "        \"SMA_diff_short_8_21\",\n",
    "        \"SMA_diff_short_21_55\",\n",
    "        \"EMA_diff_short_5_8\",\n",
    "        \"EMA_diff_short_8_21\",\n",
    "        \"EMA_diff_short_21_55\",\n",
    "        \"WMA_diff_short_5_8\",\n",
    "        \"WMA_diff_short_8_21\",\n",
    "        \"WMA_diff_short_21_55\",\n",
    "        \"SMA_diff_long_55_89\",\n",
    "        \"SMA_diff_long_55_200\",\n",
    "        \"EMA_diff_long_55_89\",\n",
    "        \"EMA_diff_long_55_200\",\n",
    "        \"WMA_diff_long_55_89\",\n",
    "        \"WMA_diff_long_55_200\",\n",
    "        \"RSI_divergence\",\n",
    "        \"MACD_acceleration\",\n",
    "        \"BB_percentage\",\n",
    "        \"volume_price_trend\",\n",
    "        \"trend_strength\",\n",
    "    ]:\n",
    "        for lag in range(1, 9):  # Same as the pivot look window\n",
    "            lagged_feature = data[col].shift(lag)\n",
    "            lagged_feature.name = f\"{col}_lag_{lag}\"\n",
    "            lagged_features.append(lagged_feature)\n",
    "\n",
    "    # Concatenate all lagged features with the original dataframe\n",
    "    lagged_features_df = pd.concat(lagged_features, axis=1)\n",
    "    data = pd.concat([data, lagged_features_df], axis=1)\n",
    "\n",
    "    # Create the label column\n",
    "    data[\"Label\"] = 0  # Initialize with 'do nothing'\n",
    "    data.loc[data[\"PivotLow\"], \"Label\"] = 1  # Buy signal\n",
    "\n",
    "    # Drop the PivotHigh and PivotLow columns\n",
    "    # data.drop(columns=[\"PivotHigh\", \"PivotLow\"], inplace=True)\n",
    "    data.drop(columns=[\"PivotLow\"], inplace=True)\n",
    "\n",
    "    # Rename \"PivotHigh\" to \"PivotHigh_lag_1\"\n",
    "    data.rename(columns={\"PivotHigh\": \"PivotHigh_lag_1\"}, inplace=True)\n",
    "\n",
    "    # Drop NaN values\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Populate 'Label' with extra 1 in pos -1 and +1 if Label is 1 since 1's are scarce, commented out for now\n",
    "    # data.loc[data[\"Label\"].shift(-1, fill_value=False) & (data[\"Label\"] == 0), \"Label\"] = 1\n",
    "    # data.loc[data[\"Label\"].shift(-2, fill_value=False) & (data[\"Label\"] == 0), \"Label\"] = 1\n",
    "    # data.loc[\n",
    "    #     data[\"Label\"].shift(1, fill_value=False) & (data[\"Label\"] == 0), \"Label\"\n",
    "    # ] = 1\n",
    "    # data.loc[data[\"Label\"].shift(2, fill_value=False) & (data[\"Label\"] == 0), \"Label\"] = 1\n",
    "\n",
    "    # Add time such as day and month\n",
    "    data[\"day_of_week_lag_1\"] = data.index.dayofweek + 1\n",
    "    data[\"day_of_month_lag_1\"] = data.index.month\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply functions to each ticker\n",
    "for ticker in TICKERS:\n",
    "    split_data[ticker] = calculate_ta_indicators(split_data[ticker].copy())\n",
    "    split_data[ticker] = perform_feature_engineering(split_data[ticker].copy())\n",
    "\n",
    "\n",
    "# %% [markdown]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Standardize the data\n",
    "\n",
    "# %%\n",
    "# Step 1: Perform train-test split first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Assuming `split_data` and `TICKERS` are already defined earlier in your code\n",
    "\n",
    "# Step 1: Perform train-test split first\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for ticker in TICKERS:\n",
    "    train_data[ticker], test_data[ticker] = train_test_split(\n",
    "        split_data[ticker], test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "# Step 2: Prepare features for scaling\n",
    "all_train_features = []\n",
    "for ticker in TICKERS:\n",
    "    features = train_data[ticker].drop(columns=[\"Label\"])\n",
    "    all_train_features.append(features)\n",
    "\n",
    "# Concatenate all training features\n",
    "combined_train_features = pd.concat(all_train_features, ignore_index=True)\n",
    "\n",
    "# Step 3: Fit the scaler on combined training features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(combined_train_features)\n",
    "\n",
    "# Step 4: Apply the scaler to each ticker's data (both train and test)\n",
    "normalized_train_data = {}\n",
    "normalized_test_data = {}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Training data\n",
    "    train_features = train_data[ticker].drop(columns=[\"Label\"])\n",
    "    train_labels = train_data[ticker][\"Label\"]\n",
    "    normalized_train_features = scaler.transform(train_features)\n",
    "    normalized_train_data[ticker] = pd.DataFrame(\n",
    "        normalized_train_features, columns=train_features.columns\n",
    "    )\n",
    "    normalized_train_data[ticker][\"Label\"] = train_labels.reset_index(drop=True)\n",
    "\n",
    "    # Test data\n",
    "    test_features = test_data[ticker].drop(columns=[\"Label\"])\n",
    "    test_labels = test_data[ticker][\"Label\"]\n",
    "    normalized_test_features = scaler.transform(test_features)\n",
    "    normalized_test_data[ticker] = pd.DataFrame(\n",
    "        normalized_test_features, columns=test_features.columns\n",
    "    )\n",
    "    normalized_test_data[ticker][\"Label\"] = test_labels.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Step 5: Concatenate the normalized training data\n",
    "combined_train_data = pd.concat(normalized_train_data.values(), ignore_index=True)\n",
    "combined_test_data = pd.concat(normalized_test_data.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "# Select the features and target\n",
    "features = [col for col in combined_train_data.columns if \"lag\" in col]\n",
    "target = \"Label\"\n",
    "X_train_all = combined_train_data[features]\n",
    "y_train = combined_train_data[target]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 7: Feature selection function, half of the features\n",
    "# def select_features(X, y, correlation_threshold=0.90, n_features=233):\n",
    "#     # Remove highly correlated features\n",
    "#     corr_matrix = X.corr().abs()\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     to_drop = [\n",
    "#         column for column in upper.columns if any(upper[column] > correlation_threshold)\n",
    "#     ]\n",
    "#     X = X.drop(to_drop, axis=1)\n",
    "\n",
    "#     # Select top features based on mutual information\n",
    "#     mi_scores = mutual_info_classif(X, y)\n",
    "#     mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "#     top_features = mi_scores.nlargest(n_features).index.tolist()\n",
    "\n",
    "#     return X[top_features], top_features\n",
    "\n",
    "\n",
    "# def stable_feature_selection(X, y, n_rounds=10):\n",
    "#     feature_counts = {col: 0 for col in X.columns}\n",
    "#     for _ in range(n_rounds):\n",
    "#         _, selected_features = select_features(X, y)\n",
    "#         for feature in selected_features:\n",
    "#             feature_counts[feature] += 1\n",
    "\n",
    "#     stable_features = [\n",
    "#         feature for feature, count in feature_counts.items() if count >= n_rounds // 2\n",
    "#     ]\n",
    "#     return X[stable_features], stable_features\n",
    "\n",
    "\n",
    "# # Step 8: Apply feature selection to the entire dataset\n",
    "# # Step 9: Apply feature selection to training data\n",
    "# X_train, selected_columns = stable_feature_selection(X_train_all, y_train)\n",
    "\n",
    "# # Step 11: Apply feature selection to test data\n",
    "# X_test = combined_test_data[selected_columns]\n",
    "# y_test = combined_test_data[target]\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ### model\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def remove_correlated_features(X, correlation_threshold=0.95):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "    return X.drop(columns=to_drop)\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=100, cv=5):\n",
    "    # Remove highly correlated features first\n",
    "    X = remove_correlated_features(X)\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    \n",
    "    # Initialize RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features_to_select, step=10)\n",
    "    \n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    feature_scores = np.zeros(X.shape[1])\n",
    "    \n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Fit RFE\n",
    "        rfe.fit(X_train, y_train)\n",
    "        \n",
    "        # Get selected features\n",
    "        feature_scores += rfe.ranking_ == 1\n",
    "        \n",
    "        # Evaluate performance\n",
    "        X_val_selected = X_val.iloc[:, rfe.support_]\n",
    "        rf.fit(X_train.iloc[:, rfe.support_], y_train)\n",
    "        y_pred = rf.predict_proba(X_val_selected)[:, 1]\n",
    "        # auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # Select features that were chosen in at least half of the folds\n",
    "    selected_features = X.columns[feature_scores >= cv // 2].tolist()\n",
    "    \n",
    "    return X[selected_features], selected_features\n",
    "\n",
    "# Apply feature selection\n",
    "n_features_to_select = min(100, X_train_all.shape[1] // 2)  # Select half of the features or 100, whichever is smaller\n",
    "X_train, selected_columns = rfe_feature_selection(X_train_all, y_train, n_features_to_select=n_features_to_select)\n",
    "\n",
    "# Apply feature selection to test data\n",
    "X_test = combined_test_data[selected_columns]\n",
    "y_test = combined_test_data[target]\n",
    "\n",
    "print(f\"Number of selected features: {len(selected_columns)}\")\n",
    "print(\"Selected features:\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import optuna\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Constants\n",
    "N_TRIALS = 15\n",
    "NB_CV = 8\n",
    "THREADS = psutil.cpu_count(logical=True)\n",
    "RANDOM_STATE = 42\n",
    "RESAMPLE = False\n",
    "\n",
    "# Define the TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=NB_CV)\n",
    "\n",
    "# SMOTETomek for handling imbalanced data\n",
    "smote_tomek = SMOTETomek(sampling_strategy=0.5, random_state=RANDOM_STATE)\n",
    "smoteenn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy=0.5)\n",
    "\n",
    "\n",
    "# Function to calculate multiple metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"average_precision\": average_precision_score(y_true, y_pred_proba),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_pred_proba),\n",
    "    }\n",
    "\n",
    "\n",
    "# Tune XGBoost Classifier\n",
    "def tune_xgb(trial):\n",
    "    params = {\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 5, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 3000),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 10, 100),\n",
    "        # \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 10, 50),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 1000),\n",
    "        \"grow_policy\": trial.suggest_categorical(\n",
    "            \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
    "        \"tree_method\": trial.suggest_categorical(\n",
    "            \"tree_method\", [\"auto\", \"exact\", \"approx\", \"hist\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        **params, random_state=RANDOM_STATE, eval_metric=\"aucpr\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # scores = cross_val_score(\n",
    "    #     model, X_train, y_train, cv=tscv, scoring=\"average_precision\", n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        if RESAMPLE:\n",
    "            X_resampled, y_resampled = smoteenn.fit_resample(X_fold_train, y_fold_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_fold_train, y_fold_train\n",
    "\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_fold_test)[:, 1]\n",
    "        # score = average_precision_score(y_fold_test, y_pred_proba)\n",
    "        score = precision_score(y_fold_test, model.predict(X_fold_test), zero_division = 0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Tune RandomForest Classifier\n",
    "def tune_rf(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 3000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.1, 1.0),\n",
    "        \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "        \"class_weight\": trial.suggest_categorical(\n",
    "            \"class_weight\", [\"balanced\", \"balanced_subsample\"]\n",
    "        ),\n",
    "        \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 0, 0.1),\n",
    "        \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0, 0.1),\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**params, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "    # scores = cross_val_score(\n",
    "    #     model, X_train, y_train, cv=tscv, scoring=\"average_precision\", n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        if RESAMPLE:\n",
    "            X_resampled, y_resampled = smoteenn.fit_resample(X_fold_train, y_fold_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_fold_train, y_fold_train\n",
    "\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_fold_test)[:, 1]\n",
    "        # score = average_precision_score(y_fold_test, y_pred_proba)\n",
    "        score = precision_score(y_fold_test, model.predict(X_fold_test), zero_division = 0)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Tune CatBoost Classifier\n",
    "def tune_catboost(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 500, 5000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 10, 100),\n",
    "        # \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 10, 50),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 10),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 10),\n",
    "        \"grow_policy\": trial.suggest_categorical(\n",
    "            \"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]\n",
    "        ),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300),\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        **params, random_seed=RANDOM_STATE, verbose=0, thread_count=THREADS\n",
    "    )\n",
    "\n",
    "    # scores = cross_val_score(\n",
    "    #     model, X_train, y_train, cv=tscv, scoring=\"average_precision\", n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        if RESAMPLE:\n",
    "            X_resampled, y_resampled = smoteenn.fit_resample(X_fold_train, y_fold_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_fold_train, y_fold_train\n",
    "\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_fold_test)[:, 1]\n",
    "        # score = average_precision_score(y_fold_test, y_pred_proba)\n",
    "        score = precision_score(y_fold_test, model.predict(X_fold_test), zero_division = 0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def tune_svm(trial):\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1, 100, log=True),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"rbf\", \"poly\"]),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-5, 1, log=True),\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [\"balanced\", None]),\n",
    "    }\n",
    "\n",
    "    model = SVC(**params, random_state=RANDOM_STATE, probability=True)\n",
    "\n",
    "    # scores = cross_val_score(\n",
    "    #     model, X_train, y_train, cv=tscv, scoring=\"average_precision\", n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        if RESAMPLE:\n",
    "            X_resampled, y_resampled = smoteenn.fit_resample(X_fold_train, y_fold_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_fold_train, y_fold_train\n",
    "\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_fold_test)[:, 1]\n",
    "        # score = average_precision_score(y_fold_test, y_pred_proba)\n",
    "        score = precision_score(y_fold_test, model.predict(X_fold_test), zero_division = 0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Optimize models\n",
    "\n",
    "# Initialize MLP model with best parameters\n",
    "study_svm = optuna.create_study(direction=\"maximize\", study_name=\"SVM\")\n",
    "study_svm.optimize(tune_svm, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost\")\n",
    "study_xgb.optimize(tune_xgb, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "study_rf = optuna.create_study(direction=\"maximize\", study_name=\"RandomForest\")\n",
    "study_rf.optimize(tune_rf, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "study_catboost = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost\")\n",
    "study_catboost.optimize(tune_catboost, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# Initialize models with best parameters\n",
    "svm_model = SVC(**study_svm.best_params, random_state=RANDOM_STATE, probability=True)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    **study_xgb.best_params,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"aucpr\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_model = RandomForestClassifier(\n",
    "    **study_rf.best_params, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "catboost_model = CatBoostClassifier(\n",
    "    **study_catboost.best_params,\n",
    "    random_seed=RANDOM_STATE,\n",
    "    verbose=0,\n",
    "    thread_count=THREADS,\n",
    ")\n",
    "\n",
    "\n",
    "# Create StackingClassifier\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", xgb_model),\n",
    "        (\"rf\", rf_model),\n",
    "        (\"catboost\", catboost_model),\n",
    "        (\"svm\", svm_model),\n",
    "    ],\n",
    "    final_estimator=xgb_model,\n",
    "    passthrough=True,\n",
    "    n_jobs=-1,\n",
    "    cv=NB_CV,\n",
    ")\n",
    "\n",
    "\n",
    "# Tune meta-model (Logistic Regression)\n",
    "def tune_meta_model(trial):\n",
    "    params = {\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 5, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 3000),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 10, 100),\n",
    "        # \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 10, 50),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 1000),\n",
    "        \"grow_policy\": trial.suggest_categorical(\n",
    "            \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
    "        \"tree_method\": trial.suggest_categorical(\n",
    "            \"tree_method\", [\"auto\", \"exact\", \"approx\", \"hist\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    stacked_model.final_estimator.set_params(**params)\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        if RESAMPLE:\n",
    "            X_resampled, y_resampled = smoteenn.fit_resample(X_fold_train, y_fold_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_fold_train, y_fold_train\n",
    "\n",
    "        stacked_model.fit(X_resampled, y_resampled)\n",
    "        y_pred_proba = stacked_model.predict_proba(X_fold_test)[:, 1]\n",
    "        # score = average_precision_score(y_fold_test, y_pred_proba)\n",
    "        score = precision_score(y_fold_test, stacked_model.predict(X_fold_test), zero_division = 0)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "study_meta = optuna.create_study(direction=\"maximize\", study_name=\"MetaModel\")\n",
    "study_meta.optimize(tune_meta_model, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Set best parameters for meta-model\n",
    "stacked_model.final_estimator.set_params(**study_meta.best_params)\n",
    "\n",
    "\n",
    "# Train final model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate the model\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    estimator=stacked_model, method=\"isotonic\", cv=\"prefit\", n_jobs=-1\n",
    ")\n",
    "calibrated_model.fit(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "print(f\"End time: {datetime.now().strftime((\"%Y-%m-%d %H:%M:%S\"))}\")\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected features\n",
    "import joblib\n",
    "\n",
    "joblib.dump(selected_columns, \"../saved_models/selected_columns_v6.pkl\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"../saved_models/scaler_v6.pkl\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(stacked_model, \"../saved_models/stacked_model_v6.pkl\")\n",
    "\n",
    "joblib.dump(calibrated_model, \"../saved_models/calibrated_model_v6.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
